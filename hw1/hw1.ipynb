{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('dict.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANT_PREFIXES = [\n",
    "    \"авиа\",\n",
    "    \"авто\",\n",
    "    \"аква\",\n",
    "    \"анти\",\n",
    "    \"анти-\",\n",
    "    \"антропо\",\n",
    "    \"архи\",\n",
    "    \"арт\",\n",
    "    \"арт-\",\n",
    "    \"астро\",\n",
    "    \"аудио\",\n",
    "    \"аэро\",\n",
    "    \"без\",\n",
    "    \"бес\",\n",
    "    \"био\",\n",
    "    \"вело\",\n",
    "    \"взаимо\",\n",
    "    \"вне\",\n",
    "    \"внутри\",\n",
    "    \"видео\",\n",
    "    \"вице-\",\n",
    "    \"вперед\",\n",
    "    \"впереди\",\n",
    "    \"гекто\",\n",
    "    \"гелио\",\n",
    "    \"гео\",\n",
    "    \"гетеро\",\n",
    "    \"гига\",\n",
    "    \"гигро\",\n",
    "    \"гипер\",\n",
    "    \"гипо\",\n",
    "    \"гомо\",\n",
    "    \"дву\",\n",
    "    \"двух\",\n",
    "    \"де\",\n",
    "    \"дез\",\n",
    "    \"дека\",\n",
    "    \"деци\",\n",
    "    \"дис\",\n",
    "    \"до\",\n",
    "    \"евро\",\n",
    "    \"за\",\n",
    "    \"зоо\",\n",
    "    \"интер\",\n",
    "    \"инфра\",\n",
    "    \"квази\",\n",
    "    \"квази-\",\n",
    "    \"кило\",\n",
    "    \"кино\",\n",
    "    \"контр\",\n",
    "    \"контр-\",\n",
    "    \"космо\",\n",
    "    \"космо-\",\n",
    "    \"крипто\",\n",
    "    \"лейб-\",\n",
    "    \"лже\",\n",
    "    \"лже-\",\n",
    "    \"макро\",\n",
    "    \"макси\",\n",
    "    \"макси-\",\n",
    "    \"мало\",\n",
    "    \"меж\",\n",
    "    \"медиа\",\n",
    "    \"медиа-\",\n",
    "    \"мега\",\n",
    "    \"мета\",\n",
    "    \"мета-\",\n",
    "    \"метео\",\n",
    "    \"метро\",\n",
    "    \"микро\",\n",
    "    \"милли\",\n",
    "    \"мини\",\n",
    "    \"мини-\",\n",
    "    \"моно\",\n",
    "    \"мото\",\n",
    "    \"много\",\n",
    "    \"мульти\",\n",
    "    \"нано\",\n",
    "    \"нарко\",\n",
    "    \"не\",\n",
    "    \"небез\",\n",
    "    \"недо\",\n",
    "    \"нейро\",\n",
    "    \"нео\",\n",
    "    \"низко\",\n",
    "    \"обер-\",\n",
    "    \"обще\",\n",
    "    \"одно\",\n",
    "    \"около\",\n",
    "    \"орто\",\n",
    "    \"палео\",\n",
    "    \"пан\",\n",
    "    \"пара\",\n",
    "    \"пента\",\n",
    "    \"пере\",\n",
    "    \"пиро\",\n",
    "    \"поли\",\n",
    "    \"полу\",\n",
    "    \"после\",\n",
    "    \"пост\",\n",
    "    \"пост-\",\n",
    "    \"порно\",\n",
    "    \"пра\",\n",
    "    \"пра-\",\n",
    "    \"пред\",\n",
    "    \"пресс-\",\n",
    "    \"противо\",\n",
    "    \"противо-\",\n",
    "    \"прото\",\n",
    "    \"псевдо\",\n",
    "    \"псевдо-\",\n",
    "    \"радио\",\n",
    "    \"разно\",\n",
    "    \"ре\",\n",
    "    \"ретро\",\n",
    "    \"ретро-\",\n",
    "    \"само\",\n",
    "    \"санти\",\n",
    "    \"сверх\",\n",
    "    \"сверх-\",\n",
    "    \"спец\",\n",
    "    \"суб\",\n",
    "    \"супер\",\n",
    "    \"супер-\",\n",
    "    \"супра\",\n",
    "    \"теле\",\n",
    "    \"тетра\",\n",
    "    \"топ-\",\n",
    "    \"транс\",\n",
    "    \"транс-\",\n",
    "    \"ультра\",\n",
    "    \"унтер-\",\n",
    "    \"штаб-\",\n",
    "    \"экзо\",\n",
    "    \"эко\",\n",
    "    \"эндо\",\n",
    "    \"эконом-\",\n",
    "    \"экс\",\n",
    "    \"экс-\",\n",
    "    \"экстра\",\n",
    "    \"экстра-\",\n",
    "    \"электро\",\n",
    "    \"энерго\",\n",
    "    \"этно\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_common(word, forms):\n",
    "    n = len(word)\n",
    "    i = 0\n",
    "    while (i < n):\n",
    "        is_ok = True\n",
    "        for form in forms:\n",
    "            if len(form) > i and form[i] == word[i]:\n",
    "                continue\n",
    "            else:\n",
    "                is_ok = False\n",
    "                break\n",
    "        if not is_ok:\n",
    "            break\n",
    "        i += 1    \n",
    "        \n",
    "    return word[:i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_part_dict = {\n",
    "    \"NOUN\" : \"S\",\n",
    "    \"ADJF\" : \"A\",\n",
    "    \"ADJS\" : \"A\",\n",
    "    \"COMP\" : \"A\",\n",
    "    \"VERB\" : \"V\",\n",
    "    \"INFN\" : \"V\",\n",
    "    \"PRTF\" : \"V\",\n",
    "    \"PRTS\" : \"V\",\n",
    "    \"GRND\" : \"V\",\n",
    "    \"NUMR\" : \"NI\",\n",
    "    \"ADVB\" : \"ADV\",\n",
    "    \"NPRO\" : \"NI\",\n",
    "    \"PRED\" : \"S\",\n",
    "    \"PREP\" : \"PR\",\n",
    "    \"CONJ\" : \"CONJ\",\n",
    "    \"PRCL\" : \"ADV\",\n",
    "    \"INTJ\" : \"ADV\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemma:\n",
    "    def __init__(self, id_, word, word_forms, speech_part):\n",
    "        self.moved = -1\n",
    "        self.word = word.lower().replace('ё', 'е') \n",
    "        self.speech_part = speech_part\n",
    "        self.id = id_\n",
    "        \n",
    "        self.end_list = [''] * len(word_forms)\n",
    "        self.pref_list = [''] * len(word_forms)\n",
    "        \n",
    "        word_forms_ = []\n",
    "        for i in range(len(word_forms)):\n",
    "            form = word_forms[i]\n",
    "            if form.startswith('по') and not word.startswith('по'):\n",
    "                form = word_forms[i][2:]\n",
    "                self.pref_list[i] = 'по'\n",
    "            if form.startswith('наи') and not word.startswith('наи'):\n",
    "                form = word_forms[i][3:]\n",
    "                self.pref_list[i] = 'наи'    \n",
    "            word_forms_.append(form.lower().replace('ё', 'е'))\n",
    "            \n",
    "        self.stem = max_common(self.word, word_forms_)\n",
    "        if (self.stem == ''):\n",
    "            self.end_list = word_forms_\n",
    "            return\n",
    "        \n",
    "        for i in range(len(word_forms_)):\n",
    "            form = word_forms_[i]\n",
    "            end = form.replace(self.stem, '', 1)\n",
    "            self.end_list[i] = end\n",
    "            #print(self.pref_list[i] + \" _ \" + self.stem + \" _ \" + end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_id_to_lemma_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391243/391243 [01:10<00:00, 5533.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for lemma in tqdm(root.find('lemmata')):\n",
    "    word = lemma.find('l').get('t')\n",
    "    #if ('-' in word):\n",
    "     #   continue\n",
    "    word_forms = []\n",
    "    for form in lemma.findall('f'):\n",
    "        word_forms.append(form.get('t'))\n",
    "    speech_part = speech_part_dict[lemma.find('l').find('g').get('v')]\n",
    "    id_ = lemma.get('id')\n",
    "    parsed_lemma = Lemma(id_, word, word_forms, speech_part) \n",
    "    lemma_id_to_lemma_dict[id_] = parsed_lemma\n",
    "    #if (parsed_lemma.stem in stem_to_lemma_dict.keys()):\n",
    "    #    stem_to_lemma_dict[parsed_lemma.stem].append(parsed_lemma)\n",
    "    #else:\n",
    "    #    stem_to_lemma_dict[parsed_lemma.stem] = [parsed_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391243"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma_id_to_lemma_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_forms_from_lemma(lemma):\n",
    "    word_forms = []\n",
    "    for i in range(len(lemma.end_list)):\n",
    "        form = lemma.pref_list[i] + lemma.stem + lemma.end_list[i] \n",
    "        word_forms.append(form)\n",
    "    return word_forms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_lemma(from_lemma, to_lemma):\n",
    "    from_forms = generate_word_forms_from_lemma(from_lemma)\n",
    "    to_forms = generate_word_forms_from_lemma(to_lemma)\n",
    "    return Lemma(to_lemma.id, to_lemma.word, from_forms + to_forms, to_lemma.speech_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_lemma(from_lemma_id, to_lemma_id):\n",
    "    while True:\n",
    "        if lemma_id_to_lemma_dict[to_lemma_id].moved != -1:\n",
    "            to_lemma_id = lemma_id_to_lemma_dict[to_lemma_id].moved\n",
    "        else:\n",
    "            break\n",
    "    from_lemma = lemma_id_to_lemma_dict[from_lemma_id]\n",
    "    from_lemma.moved = to_lemma_id\n",
    "    to_lemma = lemma_id_to_lemma_dict[to_lemma_id]\n",
    "    to_lemma = append_lemma(from_lemma, to_lemma)\n",
    "    return from_lemma, to_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258488/258488 [01:17<00:00, 3349.35it/s]\n"
     ]
    }
   ],
   "source": [
    "EXCLUDED_LINK_TYPES = ['7', '11', '21', '23', '26', '27']\n",
    "for link in tqdm(root.find('links')):\n",
    "    from_ = link.get('to')\n",
    "    to_ = link.get('from')\n",
    "    type_ = link.get('type')\n",
    "    #print(from_ == \"6\")\n",
    "    #break\n",
    "    if type_ not in EXCLUDED_LINK_TYPES:\n",
    "        lemma_id_to_lemma_dict[from_], lemma_id_to_lemma_dict[to_] = move_lemma(from_, to_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 248707/391243 [00:28<00:02, 47995.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFFFFF241938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391243/391243 [00:30<00:00, 12772.73it/s]\n"
     ]
    }
   ],
   "source": [
    "stem_to_lemma_dict = {}\n",
    "for lemma_id in tqdm(lemma_id_to_lemma_dict.keys()):\n",
    "    lemma = lemma_id_to_lemma_dict[lemma_id]\n",
    "    if lemma.word == \"побывал\" :\n",
    "        print(\"FFFFFF\" + str(lemma.moved))\n",
    "    if not lemma.moved == -1:\n",
    "        continue\n",
    "    if lemma.stem in stem_to_lemma_dict.keys():\n",
    "        stem_to_lemma_dict[lemma.stem].append(lemma)\n",
    "    else:\n",
    "        stem_to_lemma_dict[lemma.stem] = [lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('ocorp_nonmod.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "token\n",
      "token\n",
      "token\n",
      "token\n",
      "token\n",
      "token\n"
     ]
    }
   ],
   "source": [
    "for child in root.findall('text')[1].find('paragraphs').findall('paragraph')[0].findall('sentence')[0].find('tokens'):\n",
    "    print(child.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('[^а-яА-Я]+')\n",
    "\n",
    "#Out: 'abdE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSpeechPart:\n",
    "    def __init__(self, word, speech_part):\n",
    "        self.word = word\n",
    "        self.speech_part = speech_part\n",
    "    def __eq__(self, obj2):\n",
    "        return self.word == obj2.word and self.speech_part == obj2.speech_part\n",
    "    def __hash__(self):\n",
    "        return hash((self.word, self.speech_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_speech_part = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in root.findall('text'):\n",
    "    for paragraph in text.find('paragraphs').findall('paragraph'):\n",
    "        for sentence in paragraph.findall('sentence'):\n",
    "            for token in sentence.find('tokens').findall('token'):\n",
    "                text = regex.sub('', token.get('text')).lower().replace('ё', 'e')\n",
    "                if text:\n",
    "                    sp = token.find('tfr').find('v').find('l').find('g').get('v')\n",
    "                    if not sp in speech_part_dict.keys():\n",
    "                        continue\n",
    "                    sp = speech_part_dict[sp]\n",
    "                    wsp = WordSpeechPart(text, sp)\n",
    "                    if text in word_to_speech_part.keys():\n",
    "                        sp_dict = word_to_speech_part[text]\n",
    "                        if wsp in sp_dict.keys():\n",
    "                            word_to_speech_part[text][wsp] = word_to_speech_part[text][wsp] + 1\n",
    "                        else:\n",
    "                            word_to_speech_part[text][wsp] = 1\n",
    "                    else:\n",
    "                        word_to_speech_part[text] = {wsp : 1}\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_popular_speech_part = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in word_to_speech_part.keys():\n",
    "    max_wsp = list(word_to_speech_part[key].keys())[0]\n",
    "    max_cnt = word_to_speech_part[key][max_wsp]\n",
    "    #ans = \"\"\n",
    "    for wsp in word_to_speech_part[key].keys():\n",
    "            #ans += \" \" + wsp.default + \" \" + wsp.speech_part + \" \" + str(word_to_speech_part[key][wsp]) + \"   \"\n",
    "        if word_to_speech_part[key][wsp] > max_cnt:\n",
    "            max_cnt = word_to_speech_part[key][wsp]\n",
    "            max_wsp = wsp\n",
    "        #print(ans + \"MAXIMUM:\" + max_wsp.default + \" \" + max_wsp.speech_part + \" \" + str(max_cnt))    \n",
    "    word_to_popular_speech_part[key] = max_wsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45114"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_popular_speech_part.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(word):\n",
    "    word = word.lower().replace('ё', 'e')\n",
    "    longest_prefix = \"\"\n",
    "    for prefix in CONSTANT_PREFIXES:\n",
    "        if word.startswith(prefix) and len(prefix) > len(longest_prefix):\n",
    "            longest_prefix = prefix\n",
    "    if longest_prefix:\n",
    "        prf, wrd = remove_prefix(word.replace(longest_prefix, '', 1))\n",
    "        return longest_prefix + prf, wrd\n",
    "    return '', word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(prefix, stem, end, speech_part):\n",
    "    if stem in stem_to_lemma_dict.keys():\n",
    "        lemmas = stem_to_lemma_dict[stem]\n",
    "        #if stem == 'политическ':\n",
    "        #    print(lemmas[0].end_list)\n",
    "    \n",
    "        for lemma in lemmas:\n",
    "            for i in range(len(lemma.end_list)):\n",
    "                #if stem == 'политическ' and i == 7:\n",
    "                    #print(lemma.pref_list[i] + \" \" + prefix)\n",
    "                    #print(lemma.pref_list[i] == prefix)\n",
    "                if prefix == lemma.pref_list[i] and end == lemma.end_list[i]:\n",
    "                    #if stem == 'политическ':\n",
    "                    #    print(\"!!!\")\n",
    "                    if speech_part == \"\" or speech_part == lemma.speech_part:\n",
    "                        return lemma\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_extracted_prefix(prefix, word, speech_part):\n",
    "    i = 1\n",
    "    while (i <= len(word)):\n",
    "        #print(prefix + \" \" + word[:i] + \" \" + word[i:])\n",
    "        lmm = process_word(prefix, word[:i], word[i:], speech_part)\n",
    "        #if (word[:i] == 'политическ'):\n",
    "        #    print(lmm.word)\n",
    "        if lmm:\n",
    "            return lmm\n",
    "        i += 1\n",
    "    return process_word(prefix, '', word, speech_part)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"был\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = \"dataset.txt\"\n",
    "OUTPUT_FILE_NAME = \"output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONJ = [\"и\", \"а\", \"но\", \"да\", \"если\", \"что\", \"когда\"]\n",
    "PR = [\"за\", \"для\", \"в\", \"о\", \"к\", \"из\", \"от\", \"по\", \"под\", \"с\", \"об\", \"обо\", \"до\", \"над\", \"на\", \"ко\", \"к\", \"без\", \"из\", \"у\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:04, 44.31it/s]\n"
     ]
    }
   ],
   "source": [
    "results_all = \"\"\n",
    "with open(INPUT_FILE_NAME, 'r') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        results = \"\"\n",
    "        for word in line.split(' '):\n",
    "            pure_word = word.replace(',', '').replace('.', '').replace('?', '').replace('!', '').replace('\\n','')\n",
    "            if pure_word in PR:\n",
    "                results = results + pure_word + \"{\" + pure_word.lower().replace('ё', 'e') + \"=PR} \"\n",
    "                continue\n",
    "            if pure_word in CONJ:\n",
    "                results = results + pure_word + \"{\" + pure_word.lower().replace('ё', 'e') + \"=CONJ} \"\n",
    "                continue \n",
    "            speech_part = \"\"    \n",
    "            if pure_word in word_to_popular_speech_part:\n",
    "                speech_part = word_to_popular_speech_part[pure_word].speech_part        \n",
    "            if pure_word:\n",
    "                constant_prefix, pure_word_no_pref = remove_prefix(pure_word)\n",
    "        #print(constant_prefix + \" \" + pure_word_no_pref)\n",
    "                if pure_word_no_pref.startswith('по'):\n",
    "                    lmm = process_word_extracted_prefix('по', pure_word_no_pref[2:], speech_part) \n",
    "                    if lmm:\n",
    "                        results = results + pure_word + \"{\" + constant_prefix + lmm.word + \"=\" + lmm.speech_part + \"} \"\n",
    "                        continue\n",
    "                if pure_word_no_pref.startswith('наи'):\n",
    "                    lmm = process_word_extracted_prefix('наи', pure_word_no_pref[3:], speech_part) \n",
    "                    if lmm:\n",
    "                        results = results + pure_word + \"{\" + constant_prefix + lmm.word + \"=\" + lmm.speech_part + \"} \"\n",
    "                        continue   \n",
    "        #print(\"12345\")\n",
    "        #print(pure_word_no_pref)\n",
    "                lmm = process_word_extracted_prefix('', pure_word_no_pref, speech_part) \n",
    "        #if pure_word_no_pref == 'политическая':\n",
    "            #print(\"AAAA\" + lmm.word)\n",
    "        #print(stem_to_lemma_dict[lmm.stem][0].id) \n",
    "        #print(stem_to_lemma_dict[lmm.stem][1].id)    \n",
    "                if lmm:\n",
    "                    results = results + pure_word + \"{\" + constant_prefix + lmm.word + \"=\" + lmm.speech_part + \"} \"\n",
    "                    continue\n",
    "                pure_word_no_pref = pure_word.lower().replace('ё', 'e')\n",
    "                constant_prefix = \"\"\n",
    "        #print(\"753245\")\n",
    "        #print(pure_word_no_pref)\n",
    "    \n",
    "                if pure_word_no_pref.startswith('по'):\n",
    "                    lmm = process_word_extracted_prefix('по', pure_word_no_pref[2:], speech_part) \n",
    "                    if lmm:\n",
    "                        results = results + pure_word + \"{\" + constant_prefix + lmm.word + \"=\" + lmm.speech_part + \"} \"\n",
    "                        continue\n",
    "                if pure_word_no_pref.startswith('наи'):\n",
    "                    lmm = process_word_extracted_prefix('наи', pure_word_no_pref[3:], speech_part) \n",
    "                    if lmm:\n",
    "                        results = results + pure_word + \"{\" + constant_prefix + lmm.word + \"=\" + lmm.speech_part + \"} \"\n",
    "                        continue   \n",
    "        #print(\"7777\")        \n",
    "                lmm = process_word_extracted_prefix('', pure_word_no_pref, speech_part) \n",
    "        #print(\"$$$$\" + lmm.word)\n",
    "                if lmm:\n",
    "                    results = results + pure_word + \"{\" + constant_prefix + lmm.word + \"=\" + lmm.speech_part + \"} \"\n",
    "                    continue    \n",
    "                results = results + pure_word + \"{\" + constant_prefix + pure_word_no_pref + \"=ADV} \"\n",
    "        results_all = results_all + results[:-1] + \"\\n\"\n",
    "results_all = results_all[:-1]\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE_NAME, 'w') as fout:\n",
    "    fout.write(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Все{весь=NI} Гришины{гришин=A} \\n одноклассники{одноклассник=S} уже{уже=ADV} побывали{побывать=V} за{за=PR} границей{граница=S} он{он=NI} был{быть=V} чуть{чуть=ADV} ли{ли=ADV} не{не=ADV} единственным{единственный=A} кого{кто=NI} не{не=ADV} вывозили{вывозить=V} никуда{никуда=NI} дальше{далеко=ADV} Красной{красный=A} Пахры{Пахра=S}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lmm in stem_to_lemma_dict[\"\"]:\n",
    "    print(lmm.word + \" \" + str(lmm.end_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
